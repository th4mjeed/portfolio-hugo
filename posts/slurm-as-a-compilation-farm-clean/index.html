<!doctype html><html lang=en dir=auto data-theme=dark><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>SLURM as a Compilation Farm | th4mjeed</title><meta name=keywords content><meta name=description content="

Note: ðŸ“¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noobâ€™s perspective diving into it.

Introduction:
This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known."><meta name=author content><link rel=canonical href=https://th4mjeed.github.io/posts/slurm-as-a-compilation-farm-clean/><link crossorigin=anonymous href=/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css integrity="sha256-NDzEgLn/yPBMy+XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as=style><link rel=icon href=https://th4mjeed.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://th4mjeed.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://th4mjeed.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://th4mjeed.github.io/apple-touch-icon.png><link rel=mask-icon href=https://th4mjeed.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://th4mjeed.github.io/posts/slurm-as-a-compilation-farm-clean/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:url" content="https://th4mjeed.github.io/posts/slurm-as-a-compilation-farm-clean/"><meta property="og:site_name" content="th4mjeed"><meta property="og:title" content="SLURM as a Compilation Farm"><meta property="og:description" content=" Note: ðŸ“¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noobâ€™s perspective diving into it.
Introduction: This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-11-06T00:00:00+00:00"><meta property="article:modified_time" content="2025-11-06T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="SLURM as a Compilation Farm"><meta name=twitter:description content="

Note: ðŸ“¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noobâ€™s perspective diving into it.

Introduction:
This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://th4mjeed.github.io/posts/"},{"@type":"ListItem","position":2,"name":"SLURM as a Compilation Farm","item":"https://th4mjeed.github.io/posts/slurm-as-a-compilation-farm-clean/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"SLURM as a Compilation Farm","name":"SLURM as a Compilation Farm","description":" Note: ðŸ“¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noobâ€™s perspective diving into it.\nIntroduction: This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\n","keywords":[],"articleBody":" Note: ðŸ“¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noobâ€™s perspective diving into it.\nIntroduction: This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.\nLab Infrastructure: The following are all on VMware ESXI\nMaster:\nCPUs 4 Memory 4 GB Hard disk 20 GB Hostname: master Node 1:\nCPUs 4 Memory 4 GB Hard disk 40 GB Hostname: node1 Node 2:\nCPUs 8 Memory 8 GB Hard disk 40 GB Hostname: node2 Network File Storage\nSince compiling creates dozens of files, at least 30 GB is required for a successful compilation. Used the existing testing server assigned to me. NFS share path located in /mnt/slrum_share Every instance has Rocky Linux 9.5 installed with SSH, root login and defined IP of all 4 nodes in the /etc/hosts file.\nThe Architecture diagram looks like this:\nChapter 1: The installation: Install and configure dependencies\nInstallation of slurm requires EPEL repo to be installed across all instances, install and enable it via:\ndnf config-manager --set-enabled crb dnf install epel-release sudo dnf groupinstall \"Development Tools\" sudo dnf install munge munge-devel rpm-build rpmdevtools python3 gcc make openssl-devel pam-devel MUNGE is an authentication mechanism for secure communication between Slurm components. Configure it on all instances using:\nsudo useradd munge sudo mkdir -p /etc/munge /var/log/munge /var/run/munge sudo chown munge:munge /usr/local/var/run/munge sudo chmod 0755 /usr/local/var/run/munge On Master:\nsudo /usr/sbin/create-munge-key sudo chown munge:munge /etc/munge/munge.key sudo chmod 0400 /etc/munge/munge.key Copy the key to both nodes:\nscp /etc/munge/munge.key root@node1:/etc/munge/ scp /etc/munge/munge.key root@node2:/etc/munge/ Start and enable the service:\nsudo systemctl enable --now munge Installation of SLURM\nSlurm is available in the EPEL repo. Install on all 3 instances:\nsudo dnf install slurm slurm-slurmd slurm-slurmctld slurm-perlapi If by any chance packages are not available, download tar file from SchedMD Downloads, extract, compile, and install using:\nmake -j$(nproc) sudo make install Chapter 2: The Configuration: Slurm configuration\nOn all 3 instances:\nsudo useradd slurm sudo mkdir -p /etc/slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm sudo chown slurm:slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm Edit the configuration on master:\nsudo nano /etc/slurm/slurm.conf Ensure the following key lines are present and correctly configured:\nClusterName=debug SlurmUser=slurm ControlMachine=slurm-master SlurmctldPort=6817 SlurmdPort=6818 AuthType=auth/munge StateSaveLocation=/var/spool/slurmctld SlurmdSpoolDir=/var/spool/slurmd SwitchType=switch/none MpiDefault=none SlurmctldPidFile=/var/run/slurmctld.pid SlurmdPidFile=/var/run/slurmd.pid ProctrackType=proctrack/pgid ReturnToService=1 SchedulerType=sched/backfill SlurmctldTimeout=300 SlurmdTimeout=30 NodeName=node1 CPUs=4 RealMemory=3657 State=UNKNOWN NodeName=node2 CPUs=8 RealMemory=7682 State=UNKNOWN PartitionName=debug Nodes=node[1-2] Default=YES MaxTime=INFINITE State=UP Copy configuration to nodes:\nscp /etc/slurm/slurm.conf root@node1:/etc/slurm/slurm.conf scp /etc/slurm/slurm.conf root@node2:/etc/slurm/slurm.conf Start and enable services:\nsudo systemctl enable --now slurmctld sudo systemctl enable --now slurmd Firewall Configuration:\nOpen required ports:\nsudo firewall-cmd --permanent --add-port=6817/tcp sudo firewall-cmd --permanent --add-port=6818/tcp sudo firewall-cmd --permanent --add-port=6819/tcp sudo firewall-cmd --reload Chapter 3: Testing and Introduction to the commands: (While this is a short guide on the commands and its flags, you could always use man pages to understand it more deeply)\nsinfo:\nDisplays node and partition information:\nsinfo srun:\nRuns commands interactively on compute nodes:\nsrun -N2 -n2 nproc sbatch:\nSubmits a job script:\nsbatch testjob.sh squeue:\nDisplays details of currently running jobs:\nsqueue scancel:\nCancels a submitted job:\nscancel 1 scontrol:\nDisplays detailed job and node information:\nscontrol show job 1 scontrol show partition Chapter 4: Setting up the NFS storage. It is a good idea to have shared storage for SLURM. Install nfs-utils:\nsudo dnf install nfs-utils On the NFS server:\nmkdir /srv/slurm_share nano /etc/exports Add the following line:\n/srv/slurm_share 10.10.40.0/24(rw,sync,no_subtree_check,no_root_squash) Open necessary ports:\nfirewall-cmd --permanent --add-service=rpc-bind firewall-cmd --permanent --add-port={5555/tcp,5555/udp,6666/tcp,6666/udp} firewall-cmd --reload Export and enable the service:\nexportfs -v systemctl enable --now nfs-server On the master and compute nodes:\nsudo mkdir /mnt/slurm_share Add the mount in /etc/fstab:\n10.10.40.0:/srv/slurm_share /mnt/slurm_share nfs defaults 0 0 Reboot machines and verify the share mounts properly.\nChapter 5: Setting up the Compile/Build Environment Install kernel build dependencies:\nsrun -n2 -N2 sudo dnf groupinstall \"Development Tools\" -y \u0026\u0026 sudo dnf install ncurses-devel bison flex elfutils-libelf-devel openssl-devel wget bc dwarves -y Download the Linux kernel source from kernel.org:\nwget [https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz](https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz) tar xvf linux-6.14.8.tar.xz Define architecture-specific config:\nmake defconfig Create compile_kernel.sh on the shared directory:\n#!/bin/bash #SBATCH --job-name=kernel_build #SBATCH --output=kernel_build_%j.out #SBATCH --error=kernel_build_%j.err #SBATCH --time=03:00:00 #SBATCH --nodes=1 #SBATCH --cpus-per-task=8 #SBATCH --mem=8G KERNEL_SOURCE_PATH=\"/mnt/slurm_share/linux-6.8.9\" BUILD_OUTPUT_DIR=\"/mnt/slurm_share/kernel_builds/${SLURM_JOB_ID}\" mkdir -p \"$BUILD_OUTPUT_DIR\" cd \"$KERNEL_SOURCE_PATH\" NUM_MAKE_JOBS=${SLURM_CPUS_PER_TASK} make -j\"${NUM_MAKE_JOBS}\" ARCH=x86_64 Image modules dtbs if [ $? -eq 0 ]; then cp \"$KERNEL_SOURCE_PATH/arch/x86/boot/bzImage\" \"$BUILD_OUTPUT_DIR/\" else echo \"Kernel compilation failed.\" fi ","wordCount":"772","inLanguage":"en","datePublished":"2025-11-06T00:00:00Z","dateModified":"2025-11-06T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://th4mjeed.github.io/posts/slurm-as-a-compilation-farm-clean/"},"publisher":{"@type":"Organization","name":"th4mjeed","logo":{"@type":"ImageObject","url":"https://th4mjeed.github.io/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://th4mjeed.github.io/ accesskey=h title="th4mjeed (Alt + H)">th4mjeed</a><div class=logo-switches></div></div><ul id=menu><li><a href=https://th4mjeed.github.io/posts/ title=Blog><span>Blog</span></a></li><li><a href=https://th4mjeed.github.io/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://th4mjeed.github.io/about/ title=About><span>About</span></a></li><li><a href=https://th4mjeed.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">SLURM as a Compilation Farm</h1><div class=post-meta><span title='2025-11-06 00:00:00 +0000 UTC'>November 6, 2025</span></div></header><div class=post-content><hr><blockquote><p><strong>Note:</strong> ðŸ“¢ This document is based on my understanding of SLURM and is in no way a detailed guide covering every single topic. Take this as a practical guide from a noobâ€™s perspective diving into it.</p></blockquote><h2 id=introduction><strong>Introduction:</strong><a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>This guide is designed to help you effectively use the SLURM scheduler on Rocky Linux 9.5 server. The Server allows you to run computational jobs using both interactive and non-interactive modes. The goal here is to make a compilation farm, although this guide specifically focuses on compiling the Linux kernel, one should note that this may also be used to compile any other tool given that the prerequisites and dependencies are known.</p><h2 id=lab-infrastructure><strong>Lab Infrastructure:</strong><a hidden class=anchor aria-hidden=true href=#lab-infrastructure>#</a></h2><p>The following are all on VMware ESXI</p><ol><li><p><strong>Master:</strong></p><ul><li>CPUs 4</li><li>Memory 4 GB</li><li>Hard disk 20 GB</li><li>Hostname: master</li></ul></li><li><p><strong>Node 1:</strong></p><ul><li>CPUs 4</li><li>Memory 4 GB</li><li>Hard disk 40 GB</li><li>Hostname: node1</li></ul></li><li><p><strong>Node 2:</strong></p><ul><li>CPUs 8</li><li>Memory 8 GB</li><li>Hard disk 40 GB</li><li>Hostname: node2</li></ul></li><li><p><strong>Network File Storage</strong></p><ul><li>Since compiling creates dozens of files, at least 30 GB is required for a successful compilation.</li><li>Used the existing testing server assigned to me.</li><li>NFS share path located in <strong>/mnt/slrum_share</strong></li></ul></li></ol><p>Every instance has Rocky Linux 9.5 installed with SSH, root login and defined IP of all 4 nodes in the <code>/etc/hosts</code> file.</p><p>The Architecture diagram looks like this:</p><figure><img loading=lazy src=/static/images/slurm-as-a-compilation-farm/SLUM_arch.drawio.png alt="SLURM Architecture Diagram"></figure><h2 id=chapter-1-the-installation><strong>Chapter 1: The installation:</strong><a hidden class=anchor aria-hidden=true href=#chapter-1-the-installation>#</a></h2><ol><li><p><strong>Install and configure dependencies</strong></p><p>Installation of slurm requires EPEL repo to be installed across all instances, install and enable it via:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>dnf config-manager --set-enabled crb
</span></span><span style=display:flex><span>dnf install epel-release
</span></span><span style=display:flex><span>sudo dnf groupinstall <span style=color:#e6db74>&#34;Development Tools&#34;</span>
</span></span><span style=display:flex><span>sudo dnf install munge munge-devel rpm-build rpmdevtools python3 gcc make openssl-devel pam-devel</span></span></code></pre></div><p>MUNGE is an authentication mechanism for secure communication between Slurm components. Configure it on all instances using:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo useradd munge
</span></span><span style=display:flex><span>sudo mkdir -p /etc/munge /var/log/munge /var/run/munge
</span></span><span style=display:flex><span>sudo chown munge:munge /usr/local/var/run/munge
</span></span><span style=display:flex><span>sudo chmod <span style=color:#ae81ff>0755</span> /usr/local/var/run/munge</span></span></code></pre></div><p>On Master:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo /usr/sbin/create-munge-key
</span></span><span style=display:flex><span>sudo chown munge:munge /etc/munge/munge.key
</span></span><span style=display:flex><span>sudo chmod <span style=color:#ae81ff>0400</span> /etc/munge/munge.key</span></span></code></pre></div><p>Copy the key to both nodes:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>scp /etc/munge/munge.key root@node1:/etc/munge/
</span></span><span style=display:flex><span>scp /etc/munge/munge.key root@node2:/etc/munge/</span></span></code></pre></div><p>Start and enable the service:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo systemctl enable --now munge</span></span></code></pre></div></li><li><p><strong>Installation of SLURM</strong></p><p>Slurm is available in the EPEL repo. Install on all 3 instances:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo dnf install slurm slurm-slurmd slurm-slurmctld slurm-perlapi</span></span></code></pre></div><p>If by any chance packages are not available, download tar file from <a href=https://www.schedmd.com/download-slurm/>SchedMD Downloads</a>, extract, compile, and install using:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make -j<span style=color:#66d9ef>$(</span>nproc<span style=color:#66d9ef>)</span>
</span></span><span style=display:flex><span>sudo make install</span></span></code></pre></div></li></ol><h2 id=chapter-2-the-configuration><strong>Chapter 2: The Configuration:</strong><a hidden class=anchor aria-hidden=true href=#chapter-2-the-configuration>#</a></h2><ol><li><p><strong>Slurm configuration</strong></p><p>On all 3 instances:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo useradd slurm
</span></span><span style=display:flex><span>sudo mkdir -p /etc/slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm
</span></span><span style=display:flex><span>sudo chown slurm:slurm /var/spool/slurmctld /var/spool/slurmd /var/log/slurm</span></span></code></pre></div><p>Edit the configuration on master:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo nano /etc/slurm/slurm.conf</span></span></code></pre></div><p>Ensure the following key lines are present and correctly configured:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>ClusterName=debug
</span></span><span style=display:flex><span>SlurmUser=slurm
</span></span><span style=display:flex><span>ControlMachine=slurm-master
</span></span><span style=display:flex><span>SlurmctldPort=6817
</span></span><span style=display:flex><span>SlurmdPort=6818
</span></span><span style=display:flex><span>AuthType=auth/munge
</span></span><span style=display:flex><span>StateSaveLocation=/var/spool/slurmctld
</span></span><span style=display:flex><span>SlurmdSpoolDir=/var/spool/slurmd
</span></span><span style=display:flex><span>SwitchType=switch/none
</span></span><span style=display:flex><span>MpiDefault=none
</span></span><span style=display:flex><span>SlurmctldPidFile=/var/run/slurmctld.pid
</span></span><span style=display:flex><span>SlurmdPidFile=/var/run/slurmd.pid
</span></span><span style=display:flex><span>ProctrackType=proctrack/pgid
</span></span><span style=display:flex><span>ReturnToService=1
</span></span><span style=display:flex><span>SchedulerType=sched/backfill
</span></span><span style=display:flex><span>SlurmctldTimeout=300
</span></span><span style=display:flex><span>SlurmdTimeout=30
</span></span><span style=display:flex><span>NodeName=node1 CPUs=4 RealMemory=3657 State=UNKNOWN
</span></span><span style=display:flex><span>NodeName=node2 CPUs=8 RealMemory=7682 State=UNKNOWN
</span></span><span style=display:flex><span>PartitionName=debug Nodes=node[1-2] Default=YES MaxTime=INFINITE State=UP</span></span></code></pre></div><p>Copy configuration to nodes:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>scp /etc/slurm/slurm.conf root@node1:/etc/slurm/slurm.conf
</span></span><span style=display:flex><span>scp /etc/slurm/slurm.conf root@node2:/etc/slurm/slurm.conf</span></span></code></pre></div><p>Start and enable services:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo systemctl enable --now slurmctld
</span></span><span style=display:flex><span>sudo systemctl enable --now slurmd</span></span></code></pre></div></li><li><p><strong>Firewall Configuration:</strong></p><p>Open required ports:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo firewall-cmd --permanent --add-port<span style=color:#f92672>=</span>6817/tcp
</span></span><span style=display:flex><span>sudo firewall-cmd --permanent --add-port<span style=color:#f92672>=</span>6818/tcp
</span></span><span style=display:flex><span>sudo firewall-cmd --permanent --add-port<span style=color:#f92672>=</span>6819/tcp
</span></span><span style=display:flex><span>sudo firewall-cmd --reload</span></span></code></pre></div></li></ol><h2 id=chapter-3-testing-and-introduction-to-the-commands><strong>Chapter 3: Testing and Introduction to the commands:</strong><a hidden class=anchor aria-hidden=true href=#chapter-3-testing-and-introduction-to-the-commands>#</a></h2><p><strong>(While this is a short guide on the commands and its flags, you could always use man pages to understand it more deeply)</strong></p><ol><li><p><code>sinfo</code>:</p><p>Displays node and partition information:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sinfo</span></span></code></pre></div></li><li><p><code>srun</code>:</p><p>Runs commands interactively on compute nodes:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>srun -N2 -n2 nproc</span></span></code></pre></div></li><li><p><code>sbatch</code>:</p><p>Submits a job script:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sbatch testjob.sh</span></span></code></pre></div></li><li><p><code>squeue</code>:</p><p>Displays details of currently running jobs:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>squeue</span></span></code></pre></div></li><li><p><code>scancel</code>:</p><p>Cancels a submitted job:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>scancel <span style=color:#ae81ff>1</span></span></span></code></pre></div></li><li><p><code>scontrol</code>:</p><p>Displays detailed job and node information:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>scontrol show job <span style=color:#ae81ff>1</span></span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>scontrol show partition</span></span></code></pre></div></li></ol><h2 id=chapter-4-setting-up-the-nfs-storage><strong>Chapter 4: Setting up the NFS storage.</strong><a hidden class=anchor aria-hidden=true href=#chapter-4-setting-up-the-nfs-storage>#</a></h2><p>It is a good idea to have shared storage for SLURM. Install <code>nfs-utils</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo dnf install nfs-utils</span></span></code></pre></div><p><strong>On the NFS server:</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>mkdir /srv/slurm_share
</span></span><span style=display:flex><span>nano /etc/exports</span></span></code></pre></div><p>Add the following line:</p><pre tabindex=0><code>/srv/slurm_share 10.10.40.0/24(rw,sync,no_subtree_check,no_root_squash)
</code></pre><p>Open necessary ports:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>firewall-cmd --permanent --add-service<span style=color:#f92672>=</span>rpc-bind
</span></span><span style=display:flex><span>firewall-cmd --permanent --add-port<span style=color:#f92672>={</span>5555/tcp,5555/udp,6666/tcp,6666/udp<span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>firewall-cmd --reload</span></span></code></pre></div><p>Export and enable the service:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>exportfs -v
</span></span><span style=display:flex><span>systemctl enable --now nfs-server</span></span></code></pre></div><p><strong>On the master and compute nodes:</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo mkdir /mnt/slurm_share</span></span></code></pre></div><p>Add the mount in <code>/etc/fstab</code>:</p><pre tabindex=0><code>10.10.40.0:/srv/slurm_share /mnt/slurm_share nfs defaults 0 0
</code></pre><p>Reboot machines and verify the share mounts properly.</p><h2 id=chapter-5-setting-up-the-compilebuild-environment><strong>Chapter 5: Setting up the Compile/Build Environment</strong><a hidden class=anchor aria-hidden=true href=#chapter-5-setting-up-the-compilebuild-environment>#</a></h2><p>Install kernel build dependencies:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>srun -n2 -N2 sudo dnf groupinstall <span style=color:#e6db74>&#34;Development Tools&#34;</span> -y <span style=color:#f92672>&amp;&amp;</span> sudo dnf install ncurses-devel bison flex elfutils-libelf-devel openssl-devel wget bc dwarves -y</span></span></code></pre></div><p>Download the Linux kernel source from <a href=https://kernel.org>kernel.org</a>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>wget <span style=color:#f92672>[</span>https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz<span style=color:#f92672>](</span>https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.14.8.tar.xz<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>tar xvf linux-6.14.8.tar.xz</span></span></code></pre></div><p>Define architecture-specific config:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make defconfig</span></span></code></pre></div><p>Create <code>compile_kernel.sh</code> on the shared directory:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e>#!/bin/bash
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#75715e>#SBATCH --job-name=kernel_build</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --output=kernel_build_%j.out</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --error=kernel_build_%j.err</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --time=03:00:00</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --nodes=1</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --cpus-per-task=8</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#SBATCH --mem=8G</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>KERNEL_SOURCE_PATH<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;/mnt/slurm_share/linux-6.8.9&#34;</span>
</span></span><span style=display:flex><span>BUILD_OUTPUT_DIR<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;/mnt/slurm_share/kernel_builds/</span><span style=color:#e6db74>${</span>SLURM_JOB_ID<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>mkdir -p <span style=color:#e6db74>&#34;</span>$BUILD_OUTPUT_DIR<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>cd <span style=color:#e6db74>&#34;</span>$KERNEL_SOURCE_PATH<span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>NUM_MAKE_JOBS<span style=color:#f92672>=</span><span style=color:#e6db74>${</span>SLURM_CPUS_PER_TASK<span style=color:#e6db74>}</span>
</span></span><span style=display:flex><span>make -j<span style=color:#e6db74>&#34;</span><span style=color:#e6db74>${</span>NUM_MAKE_JOBS<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span> ARCH<span style=color:#f92672>=</span>x86_64 Image modules dtbs
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> <span style=color:#f92672>[</span> $? -eq <span style=color:#ae81ff>0</span> <span style=color:#f92672>]</span>; <span style=color:#66d9ef>then</span>
</span></span><span style=display:flex><span>cp <span style=color:#e6db74>&#34;</span>$KERNEL_SOURCE_PATH<span style=color:#e6db74>/arch/x86/boot/bzImage&#34;</span> <span style=color:#e6db74>&#34;</span>$BUILD_OUTPUT_DIR<span style=color:#e6db74>/&#34;</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>else</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Kernel compilation failed.&#34;</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>fi</span></span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://th4mjeed.github.io/>th4mjeed</a></span> Â·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script></body></html>